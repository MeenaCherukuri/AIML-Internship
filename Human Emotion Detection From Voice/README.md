# ğŸ™ï¸ Human Emotion Detection from Voice

Detect the **emotion behind a speakerâ€™s voice** using audio recordings and machine learning. This project uses the **RAVDESS dataset**, audio processing libraries like **Librosa**, and **Scikit-learn** for training, along with a **Streamlit UI** for live interaction.

---

## ğŸš€ Project Overview

This project builds an intelligent system that can identify human emotions like **happy**, **angry**, **sad**, and more from voice recordings using machine learning. It extracts **audio features** and trains a classifier to predict emotional states in real-time or from uploaded `.wav` files.

---

## ğŸ¯ Objective

To detect the **emotion of a speaker** using audio input by:
- Extracting meaningful features from voice
- Training an emotion classifier
- Building a UI for real-time predictions

---

## ğŸ§° Tools & Technologies

| Category       | Technology      |
|----------------|------------------|
| Programming    | Python           |
| Audio Features | Librosa          |
| ML Models      | Scikit-learn     |
| UI             | Streamlit        |
| Dataset        | RAVDESS          |

---


---

## ğŸ—‚ï¸ Dataset

- **Name**: RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)
- **Download**: [Kaggle RAVDESS Dataset](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio)

---

## âš™ï¸ How It Works

### âœ… Feature Extraction

Each audio file is processed to extract meaningful patterns using:

- ğŸµ **MFCC (Mel-Frequency Cepstral Coefficients)**: Captures the timbral texture of the voice.
- ğŸ¼ **Chroma Features**: Captures pitch class content (related to musical elements).
- ğŸ“Š **Mel Spectrogram**: Represents the energy of frequencies over time.

These features are combined into a single feature vector per audio sample and used for training.


### âœ… Classifier
- Trained using **Random Forest** or **SVM**

### âœ… Streamlit UI
- Record or upload voice
- Predict emotion
- Display result in UI

---

## ğŸ“ˆ Model Performance

| Metric     | Value    |
|------------|----------|
| Accuracy   | ~85â€“90%  |
| Classes    | Neutral, Calm, Happy, Sad, Angry, Fearful, Disgust, Surprised |

---

## ğŸ§ª How to Run Locally (Colab + Streamlit)

### 1ï¸âƒ£ Setup in Google Colab

    ```python
     # Install dependencies
     !pip install librosa soundfile scikit-learn kaggle resampy
# Upload kaggle.json and download dataset
from google.colab import files
files.upload()  # Upload your kaggle.json

### 2ï¸âƒ£ Train the Model

    ```bash
    python train_model.py

### 3ï¸âƒ£ Run the Streamlit App
    ```bash
    streamlit run app.py

## ğŸŒ Streamlit Web App Features

- ğŸ¤ **Upload your voice or record live**
- ğŸ§  **Get predicted emotion instantly**
- ğŸ“Š **(Optional) Visualize emotion trend over time**
  

## ğŸ“ Future Enhancements

- ğŸ§ **Real-time microphone input**
- ğŸ“ˆ **Emotion trend chart**
- ğŸ” **Use deep learning models (CNN, LSTM)**
- ğŸ“± **Mobile/web deployment**

## ğŸ” Project Motivation

In human communication, tone and emotion play a crucial role alongside the spoken words. With the rise of AI and voice-driven interfaces, emotion detection from speech has become essential in fields like:

- ğŸ§ Voice assistants (e.g., Alexa, Siri)
- ğŸ§  Mental health analysis
- ğŸ“ E-learning engagement tracking
- ğŸ® Emotion-aware games
- ğŸ¤– Human-robot interaction

This project aims to bridge that emotional gap by training a machine learning model to recognize emotions from audio using classical ML techniques.

## ğŸ”„ Possible Model Improvements

- ğŸ” Try different classifiers (e.g., XGBoost, Gradient Boosting)
- ğŸ“¦ Perform hyperparameter tuning using GridSearchCV or Optuna
- ğŸ§  Replace classical ML with deep learning (CNNs, RNNs, LSTMs)
- ğŸ·ï¸ Use data augmentation techniques (noise addition, pitch shift) to increase robustness
- ğŸ” Use transfer learning from pretrained audio models (e.g., YAMNet, Wav2Vec)

## ğŸ™Œ Acknowledgements

- ğŸµ [RAVDESS Dataset (Official)](https://zenodo.org/record/1188976)
- ğŸ“‚ [Kaggle RAVDESS Source](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio)


## ğŸ§‘â€ğŸ’» How to Contribute

If you'd like to improve this project, you're welcome to contribute! Here's how:

1. Fork the repository
2. Create a new branch (`git checkout -b feature-new`)
3. Make your changes and commit (`git commit -am 'Add new feature'`)
4. Push to the branch (`git push origin feature-new`)
5. Create a new Pull Request


## ğŸ“¡ Real-Time Applications

- ğŸ§  **Mental Health Monitoring**: Detect emotional trends over time from call center data or therapy sessions.
- ğŸ“ **Call Center Optimization**: Recognize customer frustration or satisfaction during live calls.
- ğŸ—£ï¸ **Virtual Assistant Personalization**: Adjust responses based on user's emotional state.
- ğŸ“š **EdTech**: Measure student engagement during online learning.

## ğŸ§ª Suggested Advanced Features

- ğŸ”´ Real-time live microphone-based emotion detection (using WebRTC or `sounddevice`)
- ğŸ”‰ Multi-language emotion support (train on multilingual audio datasets)
- ğŸ“± Deploy as a mobile app using Streamlit + WebView or React Native
- ğŸ§µ Add audio visualization using `matplotlib` or `plotly` in Streamlit
- ğŸ—ƒï¸ Add a database layer (e.g., SQLite/MongoDB) to store user audio and results

## Results Snapshot

| Emotion     | Precision | Recall | F1-Score |
|-------------|-----------|--------|----------|
| Happy       | 0.89      | 0.86   | 0.87     |
| Angry       | 0.91      | 0.88   | 0.89     |
| Sad         | 0.88      | 0.87   | 0.87     |
| Calm        | 0.85      | 0.84   | 0.84     |
| **Average** | **0.88**  | **0.86** | **0.87** |



